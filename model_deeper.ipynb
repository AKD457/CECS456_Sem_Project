{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01e8f4d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T09:55:27.626493Z",
     "start_time": "2025-12-15T09:55:27.623676Z"
    }
   },
   "outputs": [],
   "source": [
    "#Model B \n",
    "# CNN with Deeper Architecture and Regularization\n",
    "# Uses Augmentation for training\n",
    "# Uses Deeper Model with BatchNorm and Droput and GAP\n",
    "# Uses Optimizer Weight Decay for Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76004a20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T10:23:25.462264Z",
     "start_time": "2025-12-15T10:23:25.455884Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, random, numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28a80985",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T10:16:28.788810Z",
     "start_time": "2025-12-15T10:16:28.785349Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded split sizes: 4829 1034 1036\n"
     ]
    }
   ],
   "source": [
    "# Paths and Loading Splits\n",
    "DATA_DIR = \"C:/Users/Repti/OneDrive/Documents/GitHub/CECS456_Sem_Project/natural_images\"\n",
    "SPLIT_FILE = \"splits.npz\"\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "splits = np.load(SPLIT_FILE)\n",
    "train_idx = splits[\"train_idx\"]\n",
    "val_idx   = splits[\"val_idx\"]\n",
    "test_idx  = splits[\"test_idx\"]\n",
    "\n",
    "print(\"Loaded split sizes:\", len(train_idx), len(val_idx), len(test_idx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42fdff11",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T10:16:30.339306Z",
     "start_time": "2025-12-15T10:16:30.286112Z"
    }
   },
   "outputs": [],
   "source": [
    "#Transform and DataLoaders\n",
    "\n",
    "class SubsetWithTransform(Dataset):\n",
    "    def __init__(self, dataset, indices, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.indices = list(indices)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        x, y = self.dataset[self.indices[i]]\n",
    "        if self.transform is not None:\n",
    "            x = self.transform(x)\n",
    "        return x, y\n",
    "\n",
    "def subset_with_transform(data_dir, indices, transform):\n",
    "    base = ImageFolder(root=data_dir)  # no transform here\n",
    "    return SubsetWithTransform(base, indices, transform=transform)\n",
    "\n",
    "train_tfms_B = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
    "])\n",
    "\n",
    "eval_tfms = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
    "])\n",
    "\n",
    "base_ds = datasets.ImageFolder(DATA_DIR, transform=eval_tfms)\n",
    "class_names = base_ds.classes\n",
    "num_classes = len(class_names)\n",
    "\n",
    "train_ds = subset_with_transform(DATA_DIR, train_idx, train_tfms_B)\n",
    "val_ds   = subset_with_transform(DATA_DIR, val_idx, eval_tfms)\n",
    "test_ds  = subset_with_transform(DATA_DIR, test_idx, eval_tfms)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "test_loader  = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8ea5ef8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T10:16:33.181267Z",
     "start_time": "2025-12-15T10:16:33.171895Z"
    }
   },
   "outputs": [],
   "source": [
    "#CNN with Deeper Architecture and Regularization\n",
    "class DeeperRegCNN(nn.Module):\n",
    "    def __init__(self, num_classes, dropout=0.4):\n",
    "        super().__init__()\n",
    "\n",
    "        def block(in_ch, out_ch):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "                nn.BatchNorm2d(out_ch),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "                nn.BatchNorm2d(out_ch),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(2),\n",
    "            )\n",
    "\n",
    "        self.b1 = block(3, 32)\n",
    "        self.b2 = block(32, 64)\n",
    "        self.b3 = block(64, 128)\n",
    "        self.b4 = block(128, 256)\n",
    "\n",
    "        self.gap = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.b1(x)\n",
    "        x = self.b2(x)\n",
    "        x = self.b3(x)\n",
    "        x = self.b4(x)\n",
    "        x = self.gap(x)\n",
    "        return self.head(x)\n",
    "\n",
    "model = DeeperRegCNN(num_classes, dropout=0.4).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c17f7653755d3abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training and Evaluation Functions\n",
    "def run_epoch(model, loader, optimizer=None):\n",
    "    is_train = optimizer is not None\n",
    "    model.train(is_train)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    total_loss, total_correct, total_n = 0.0, 0, 0\n",
    "\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        if is_train:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        logits = model(x)\n",
    "        loss = loss_fn(logits, y)\n",
    "\n",
    "        if is_train:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        total_correct += (logits.argmax(1) == y).sum().item()\n",
    "        total_n += x.size(0)\n",
    "\n",
    "    return total_loss / total_n, total_correct / total_n\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_all(model, loader):\n",
    "    model.eval()\n",
    "    ys, preds = [], []\n",
    "    for x, y in loader:\n",
    "        x = x.to(device)\n",
    "        logits = model(x)\n",
    "        p = logits.argmax(1).cpu().numpy()\n",
    "        preds.extend(p)\n",
    "        ys.extend(y.numpy())\n",
    "    return np.array(ys), np.array(preds)\n",
    "\n",
    "def plot_history(hist, title):\n",
    "    plt.figure()\n",
    "    plt.plot(hist[\"train_loss\"], label=\"train_loss\")\n",
    "    plt.plot(hist[\"val_loss\"], label=\"val_loss\")\n",
    "    plt.legend()\n",
    "    plt.title(title + \" - Loss\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(hist[\"train_acc\"], label=\"train_acc\")\n",
    "    plt.plot(hist[\"val_acc\"], label=\"val_acc\")\n",
    "    plt.legend()\n",
    "    plt.title(title + \" - Accuracy\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41504154",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-12-15T10:23:41.814004Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Repti\\OneDrive\\Documents\\GitHub\\CECS456_Sem_Project\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "#Trainging with Weight Decay\n",
    "EPOCHS = 25\n",
    "LR = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "history = {\"train_loss\":[], \"train_acc\":[], \"val_loss\":[], \"val_acc\":[]}\n",
    "best_val = -1\n",
    "best_state = None\n",
    "patience = 5\n",
    "bad_epochs = 0\n",
    "\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    tr_loss, tr_acc = run_epoch(model, train_loader, optimizer=optimizer)\n",
    "    va_loss, va_acc = run_epoch(model, val_loader, optimizer=None)\n",
    "\n",
    "    history[\"train_loss\"].append(tr_loss)\n",
    "    history[\"train_acc\"].append(tr_acc)\n",
    "    history[\"val_loss\"].append(va_loss)\n",
    "    history[\"val_acc\"].append(va_acc)\n",
    "\n",
    "    improved = va_acc > best_val\n",
    "    if improved:\n",
    "        best_val = va_acc\n",
    "        best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "        bad_epochs = 0\n",
    "    else:\n",
    "        bad_epochs += 1\n",
    "\n",
    "    print(f\"Epoch {ep:02d} | train {tr_loss:.4f}/{tr_acc:.4f} | val {va_loss:.4f}/{va_acc:.4f}\")\n",
    "\n",
    "    if bad_epochs >= patience:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "model.load_state_dict(best_state)\n",
    "plot_history(history, \"Model B (Deeper + Regularization)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2816c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Evaluation and Metrics\n",
    "y_true, y_pred = predict_all(model, test_loader)\n",
    "test_acc = (y_true == y_pred).mean()\n",
    "print(\"Model B Test Accuracy:\", round(float(test_acc), 4))\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_true, y_pred, target_names=class_names))\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(cm)\n",
    "plt.title(\"Model B - Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
